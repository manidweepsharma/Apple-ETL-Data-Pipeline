# Apple-ETL-Data-Pipeline

Project Objective

Business Problem:

Analyze customer and product data (inspired by Apple Inc.) to answer real-world business questions, such as:

1. Which customers purchased AirPods after buying an iPhone?

2. Which customers bought both AirPods and iphone?

3. What is the percentage of customers who bought both AirPods and iPhone? (Inprogress)

4. What is the average time between iPhone and AirPods purchases?(Inprogress)

5. What are the top-selling products in each category?(Inprogress)

Technical Goals:

1. Build an end-to-end ETL pipeline using PySpark.

2. Handle multiple data sources (CSV, Parquet, Delta Table).
   
3. Apply advanced Spark transformations and optimizations.

4. Load processed data into both Data Lake and Lakehouse architectures.

5. Follow modular, production-grade coding practices.

Tools & Technologies
PySpark (Apache Spark with Python)

Databricks Community Edition

Delta Lake

Parquet

Git (for version control)

Jupyter/Databricks Notebooks (for exploration and documentation)
